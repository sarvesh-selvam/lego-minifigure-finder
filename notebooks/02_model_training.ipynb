{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81bd7e2b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02a6543a",
   "metadata": {},
   "source": [
    "## Data Processing & Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49c58462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/data.py\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms as T\n",
    "\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
    "\n",
    "\n",
    "\n",
    "def _tfms(image_size=224):\n",
    "    train = T.Compose([\n",
    "        T.Resize(int(image_size * 1.14)),\n",
    "        T.RandomResizedCrop(image_size, scale=(0.8, 1.0)),\n",
    "        T.RandomHorizontalFlip(p=0.5),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "    ])\n",
    "    eval_ = T.Compose([\n",
    "        T.Resize(int(image_size * 1.14)),\n",
    "        T.CenterCrop(image_size),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "    ])\n",
    "    return {\"train\": train, \"val\": eval_, \"test\": eval_}\n",
    "\n",
    "\n",
    "\n",
    "class CSVDataset(Dataset):\n",
    "    def __init__(self, csv_path, img_dir, transforms=None):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        if not {\"filename\",\"label\"}.issubset(self.df.columns):\n",
    "            raise ValueError(f\"{csv_path} must have 'filename' and 'label' columns.\")\n",
    "        self.img_dir = img_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self): return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        path = os.path.join(self.img_dir, str(row[\"filename\"]))\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        y = int(row[\"label\"])\n",
    "        if self.transforms: img = self.transforms(img)\n",
    "        return img, y\n",
    "\n",
    "\n",
    "\n",
    "def make_loaders(\n",
    "    data_dir=\"data\",\n",
    "    images_subdir=\"images\",\n",
    "    batch_size=32,\n",
    "    num_workers=0,         # macOS: start at 0; bump later if stable\n",
    "    image_size=224,\n",
    "):\n",
    "    csvs = {\n",
    "        \"train\": os.path.join(data_dir, \"train.csv\"),\n",
    "        \"val\":   os.path.join(data_dir, \"val.csv\"),\n",
    "        \"test\":  os.path.join(data_dir, \"test.csv\"),\n",
    "    }\n",
    "    img_dir = os.path.join(data_dir, images_subdir)\n",
    "    tf = _tfms(image_size)\n",
    "\n",
    "    ds = {\n",
    "        split: CSVDataset(csvs[split], img_dir, transforms=tf[\"train\" if split==\"train\" else \"val\"])\n",
    "        for split in [\"train\",\"val\",\"test\"]\n",
    "    }\n",
    "\n",
    "    loaders = {\n",
    "        \"train\": DataLoader(ds[\"train\"], batch_size=batch_size, shuffle=True,\n",
    "                            num_workers=num_workers),\n",
    "        \"val\":   DataLoader(ds[\"val\"], batch_size=batch_size, shuffle=False,\n",
    "                            num_workers=num_workers),\n",
    "        \"test\":  DataLoader(ds[\"test\"], batch_size=batch_size, shuffle=False,\n",
    "                            num_workers=num_workers),\n",
    "    }\n",
    "    return loaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fd42d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 224, 224]) tensor([0, 0, 1, 0, 0, 0, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"/Users/sarvesh/Desktop/GitHub/lego-minifigure-finder/data\"\n",
    "\n",
    "loaders = make_loaders(data_dir=data_dir, images_subdir=\"images\", batch_size=16)\n",
    "imgs, labels = next(iter(loaders[\"train\"]))\n",
    "print(imgs.shape, labels[:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3f3c45",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2840a246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Use CUDA if available; otherwise prefer Apple MPS if present; else CPU.\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fcb1c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 30 batches\n",
      "val 6 batches\n",
      "test 7 batches\n"
     ]
    }
   ],
   "source": [
    "# Point to your data directory\n",
    "DATA_DIR = \"/Users/sarvesh/Desktop/GitHub/lego-minifigure-finder/data\"          # <- adjust if needed\n",
    "IMAGES_SUBDIR = \"/Users/sarvesh/Desktop/GitHub/lego-minifigure-finder/data/images\"   # <- adjust if needed\n",
    "\n",
    "# # If you created make_loaders earlier:\n",
    "# try:\n",
    "#     from src.data import make_loaders\n",
    "# except Exception:\n",
    "#     # Fallback: assume data.py at repo root\n",
    "#     from data import make_loaders\n",
    "\n",
    "# Build loaders; no pin_memory, num_workers=0 is safest on macOS\n",
    "loaders = make_loaders(\n",
    "    data_dir=DATA_DIR,\n",
    "    images_subdir=IMAGES_SUBDIR,\n",
    "    batch_size=32,\n",
    "    num_workers=0,          # bump to 2-4 later if stable\n",
    "    image_size=224\n",
    ")\n",
    "for k,v in loaders.items():\n",
    "    print(k, len(v), \"batches\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "811de34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42):\n",
    "    import random, numpy as np, torch\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    preds_all, y_all = [], []\n",
    "\n",
    "    for imgs, y in loader:\n",
    "        imgs = imgs.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(imgs)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        preds_all.append(preds.detach().cpu())\n",
    "        y_all.append(y.detach().cpu())\n",
    "\n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    y_true = torch.cat(y_all).numpy()\n",
    "    y_pred = torch.cat(preds_all).numpy()\n",
    "    epoch_acc = accuracy_score(y_true, y_pred)\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    preds_all, y_all = [], []\n",
    "\n",
    "    for imgs, y in loader:\n",
    "        imgs = imgs.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        logits = model(imgs)\n",
    "        loss = criterion(logits, y)\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        preds_all.append(preds.detach().cpu())\n",
    "        y_all.append(y.detach().cpu())\n",
    "\n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    y_true = torch.cat(y_all).numpy()\n",
    "    y_pred = torch.cat(preds_all).numpy()\n",
    "    epoch_acc = accuracy_score(y_true, y_pred)\n",
    "    return epoch_loss, epoch_acc, y_true, y_pred\n",
    "\n",
    "def fit(model, loaders, epochs, lr=1e-3, weight_decay=0.0):\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    best_val_acc = -1\n",
    "    best_state = None\n",
    "    history = []\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        t0 = time.time()\n",
    "        tr_loss, tr_acc = train_one_epoch(model, loaders[\"train\"], criterion, optimizer, device)\n",
    "        va_loss, va_acc, _, _ = evaluate(model, loaders[\"val\"], criterion, device)\n",
    "        dt = time.time() - t0\n",
    "\n",
    "        history.append({\"epoch\": epoch, \"train_loss\": tr_loss, \"train_acc\": tr_acc,\n",
    "                        \"val_loss\": va_loss, \"val_acc\": va_acc, \"sec\": dt})\n",
    "        print(f\"[{epoch:02d}] \"\n",
    "              f\"train_loss={tr_loss:.4f} acc={tr_acc:.4f} | \"\n",
    "              f\"val_loss={va_loss:.4f} acc={va_acc:.4f} | {dt:.1f}s\")\n",
    "\n",
    "        if va_acc > best_val_acc:\n",
    "            best_val_acc = va_acc\n",
    "            best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5905a0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SmallCNN(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (10): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Dropout(p=0.25, inplace=False)\n",
       "    (2): Linear(in_features=256, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SmallCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Very small CNN for quick iteration.\n",
    "    Input: [B, 3, 224, 224] -> 2 classes\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1),  # 112x112\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1), # 56x56\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),# 28x28\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),# 14x14\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.AdaptiveAvgPool2d((1,1))  # -> [B, 256, 1, 1]\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "small_cnn = SmallCNN(num_classes=2)\n",
    "small_cnn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "832963c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01] train_loss=0.3656 acc=0.8459 | val_loss=0.3259 acc=0.8871 | 8.8s\n",
      "[02] train_loss=0.3551 acc=0.8375 | val_loss=0.2697 acc=0.8978 | 8.4s\n",
      "[03] train_loss=0.3143 acc=0.8711 | val_loss=0.2370 acc=0.9032 | 8.2s\n",
      "[04] train_loss=0.3159 acc=0.8627 | val_loss=0.2624 acc=0.8763 | 8.3s\n",
      "[05] train_loss=0.3083 acc=0.8658 | val_loss=0.2327 acc=0.8925 | 8.5s\n",
      "[06] train_loss=0.2896 acc=0.8690 | val_loss=0.2802 acc=0.8602 | 8.7s\n",
      "[07] train_loss=0.3034 acc=0.8774 | val_loss=0.2288 acc=0.8978 | 8.2s\n",
      "[08] train_loss=0.2867 acc=0.8679 | val_loss=0.2742 acc=0.8925 | 8.3s\n",
      "[09] train_loss=0.2852 acc=0.8805 | val_loss=0.2222 acc=0.8871 | 8.6s\n",
      "[10] train_loss=0.3095 acc=0.8595 | val_loss=0.4176 acc=0.8387 | 8.4s\n"
     ]
    }
   ],
   "source": [
    "small_cnn, history_cnn = fit(\n",
    "    small_cnn,\n",
    "    loaders,\n",
    "    epochs=10,           # start small; increase if improving\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3b2f6b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SmallCNN Test: loss=0.4462, acc=0.8148\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8129    0.9456    0.8742       147\n",
      "           1     0.8222    0.5362    0.6491        69\n",
      "\n",
      "    accuracy                         0.8148       216\n",
      "   macro avg     0.8175    0.7409    0.7617       216\n",
      "weighted avg     0.8159    0.8148    0.8023       216\n",
      "\n",
      "Confusion matrix:\n",
      " [[139   8]\n",
      " [ 32  37]]\n"
     ]
    }
   ],
   "source": [
    "crit = nn.CrossEntropyLoss()\n",
    "test_loss, test_acc, y_true, y_pred = evaluate(small_cnn, loaders[\"test\"], crit, device)\n",
    "print(f\"SmallCNN Test: loss={test_loss:.4f}, acc={test_acc:.4f}\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1f646f",
   "metadata": {},
   "source": [
    "### ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90827e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /Users/sarvesh/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44.7M/44.7M [00:03<00:00, 12.0MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import models\n",
    "\n",
    "# PyTorch 2.x API: use \"weights\" for pretrained\n",
    "resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# Replace final layer for 2 classes\n",
    "in_feats = resnet.fc.in_features\n",
    "resnet.fc = nn.Linear(in_feats, 2)\n",
    "\n",
    "# Option 1 (recommended to start): fine-tune ALL layers\n",
    "for p in resnet.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "resnet.to(device)\n",
    "resnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e21c9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01] train_loss=0.1731 acc=0.9193 | val_loss=0.1347 acc=0.9785 | 17.5s\n",
      "[02] train_loss=0.0527 acc=0.9769 | val_loss=0.0274 acc=0.9892 | 13.2s\n",
      "[03] train_loss=0.0553 acc=0.9790 | val_loss=0.0400 acc=0.9892 | 13.3s\n",
      "[04] train_loss=0.0283 acc=0.9843 | val_loss=0.0026 acc=1.0000 | 13.3s\n",
      "[05] train_loss=0.0283 acc=0.9927 | val_loss=0.0112 acc=0.9946 | 13.0s\n"
     ]
    }
   ],
   "source": [
    "resnet, history_resnet = fit(\n",
    "    resnet,\n",
    "    loaders,\n",
    "    epochs=5,         # try 5–15; watch val accuracy\n",
    "    lr=3e-4,          # slightly smaller LR for pretrained nets\n",
    "    weight_decay=1e-4\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "657be490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet18 Test: loss=0.0835, acc=0.9630\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9484    1.0000    0.9735       147\n",
      "           1     1.0000    0.8841    0.9385        69\n",
      "\n",
      "    accuracy                         0.9630       216\n",
      "   macro avg     0.9742    0.9420    0.9560       216\n",
      "weighted avg     0.9649    0.9630    0.9623       216\n",
      "\n",
      "Confusion matrix:\n",
      " [[147   0]\n",
      " [  8  61]]\n"
     ]
    }
   ],
   "source": [
    "crit = nn.CrossEntropyLoss()\n",
    "test_loss, test_acc, y_true, y_pred = evaluate(resnet, loaders[\"test\"], crit, device)\n",
    "print(f\"ResNet18 Test: loss={test_loss:.4f}, acc={test_acc:.4f}\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee686264",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lego-mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
